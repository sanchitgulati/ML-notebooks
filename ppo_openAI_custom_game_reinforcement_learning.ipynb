{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchitgulati/ML-notebooks/blob/main/ppo_openAI_custom_game_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc99a98",
      "metadata": {
        "id": "4bc99a98"
      },
      "source": [
        "# Train an AI to play a game of yourÂ own with Stable-Baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff7e60e",
      "metadata": {
        "id": "8ff7e60e"
      },
      "source": [
        "## Define the game environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d89324",
      "metadata": {
        "id": "f6d89324"
      },
      "source": [
        "Here we define the game we want the AI to play using the OpenAI Gym class gym.env format. In the example here we set up a Snake game with two walls in the middle.\n",
        "We further need to define 3 things here:\n",
        "1. What information does the player have about the state of the game (i.e., observation space)?\n",
        "2. What possible actions the player can take (i.e., action space)?\n",
        "3. What is the reward scheme?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "id": "EkfNTBGBmKjK"
      },
      "id": "EkfNTBGBmKjK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fbf7557b",
      "metadata": {
        "id": "fbf7557b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "class Snake_game(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment for Stable Baseline 3 for the classic Snake \n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['console','rgb_array']}\n",
        "    #Direction constants\n",
        "    n_actions = 3 #3 possible steps each turn\n",
        "    LEFT = 0\n",
        "    STRAIGHT = 1\n",
        "    RIGHT = 2\n",
        "    #Grid label constants\n",
        "    EMPTY = 0\n",
        "    SNAKE = 1\n",
        "    WALL = 2\n",
        "    FOOD = 3\n",
        "    #Rewards\n",
        "    #REWARD_PER_STEP = 0 # reward for every step taken, gets into infinite loops if >0\n",
        "    #Define Max steps to avoid infinite loops\n",
        "    REWARD_WALL_HIT = -20 #should be lower than -REWARD_PER_STEP_TOWARDS_FOOD to avoid hitting wall intentionally\n",
        "    REWARD_PER_STEP_TOWARDS_FOOD = 1 #give reward for moving towards food and penalty for moving away\n",
        "    REWARD_PER_FOOD = 50 \n",
        "    MAX_STEPS_AFTER_FOOD = 200 #stop if we go too long without food to avoid infinite loops\n",
        "\n",
        "\n",
        "    def __init__(self, grid_size=12):\n",
        "        super(Snake_game, self).__init__()\n",
        "        \n",
        "        #Steps so far\n",
        "        self.stepnum = 0; self.last_food_step=0\n",
        "        \n",
        "        # Size of the 2D grid (including walls)\n",
        "        self.grid_size = grid_size\n",
        "        \n",
        "        # Initialize the snake\n",
        "        self.snake_coordinates = [ (1,1), (2,1) ] #Start in lower left corner\n",
        "        \n",
        "        #Init the grid\n",
        "        self.grid = np.zeros( (self.grid_size, self.grid_size) ,dtype=np.uint8) + self.EMPTY\n",
        "\n",
        "\n",
        "        self.grid[0,:] = self.WALL; self.grid[:,0] = self.WALL; #wall at the egdes\n",
        "\n",
        "        # self.grid[int(grid_size/2),3:(grid_size-3)] = self.WALL; #inner wall to make the game harder\n",
        "        # self.grid[4:(grid_size-4),int(grid_size/2-1)] = self.WALL; #inner wall to make the game harder\n",
        "        # #self.grid[int(grid_size/2),2:(grid_size-2)] = self.WALL; #inner wall to make the game harder\n",
        "        # self.grid[self.grid_size-1,:] = self.WALL; self.grid[:,self.grid_size-1] = self.WALL\n",
        "        \n",
        "        #put snake on grid\n",
        "        for coord in self.snake_coordinates:\n",
        "            self.grid[ coord ] = self.SNAKE  \n",
        "        \n",
        "        \n",
        "        #Start in upper right corner\n",
        "        self.grid[3,3] = self.FOOD\n",
        "\n",
        "\n",
        "\n",
        "        #Init distance to food\n",
        "        self.head_dist_to_food = self.grid_distance(self.snake_coordinates[-1],np.argwhere(self.grid==self.FOOD)[0] )\n",
        "\n",
        "\n",
        "        #Store init values\n",
        "        self.init_grid = self.grid.copy()\n",
        "        self.init_snake_coordinates = self.snake_coordinates.copy()\n",
        "        \n",
        "        # The action space\n",
        "        self.action_space = spaces.Discrete(self.n_actions)\n",
        "        # The observation space, \"position\" is the coordinates of the head; \"direction\" is which way the snake is heading, \"grid\" contains the full grid info\n",
        "        self.observation_space = gym.spaces.Dict(\n",
        "            spaces={\n",
        "                \"position\": gym.spaces.Box(low=0, high=(self.grid_size-1), shape=(2,), dtype=np.int32),\n",
        "                \"direction\": gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),\n",
        "                \"grid\": gym.spaces.Box(low = 0, high = 3, shape = (self.grid_size, self.grid_size), dtype=np.uint8),\n",
        "            })\n",
        "    \n",
        "    def grid_distance(self,pos1,pos2):\n",
        "        return np.linalg.norm(np.array(pos1,dtype=np.float32)-np.array(pos2,dtype=np.float32))\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset to initial positions\n",
        "        self.stepnum = 0; self.last_food_step=0\n",
        "        self.grid = self.init_grid.copy()\n",
        "        self.snake_coordinates = self.init_snake_coordinates.copy()\n",
        "        #Init distance to food\n",
        "        self.head_dist_to_food = self.grid_distance(self.snake_coordinates[-1],np.argwhere(self.grid==self.FOOD)[0] )\n",
        "        return self._get_obs()    \n",
        "             \n",
        "    def _get_obs(self):\n",
        "            direction = np.array(self.snake_coordinates[-1]) - np.array(self.snake_coordinates[-2])\n",
        "            #return observation in the format of self.observation_space\n",
        "            return {\"position\": np.array(self.snake_coordinates[-1],dtype=np.int32),\n",
        "                    \"direction\" : direction.astype(np.int32),\n",
        "                    \"grid\": self.grid}                  \n",
        "            \n",
        "    def step(self, action):\n",
        "        #Get direction for snake\n",
        "        direction = np.array(self.snake_coordinates[-1]) - np.array(self.snake_coordinates[-2])\n",
        "        if action == self.STRAIGHT:\n",
        "            step = direction #step in the firection the snake faces\n",
        "        elif action == self.RIGHT:\n",
        "            step = np.array( [direction[1], -direction[0]] )  #turn right\n",
        "        elif action == self.LEFT:\n",
        "            step = np.array( [-direction[1], direction[0]] )   #turn left\n",
        "        else:\n",
        "            raise ValueError(\"Action=%d is not part of the action space\"%(action))\n",
        "        #New head coordinate\n",
        "        new_coord = (np.array(self.snake_coordinates[-1]) + step).astype(np.int32)\n",
        "        #grow snake     \n",
        "        self.snake_coordinates.append( (new_coord[0],new_coord[1]) ) #convert to tuple so we can use it to index\n",
        "\n",
        "        \n",
        "        #Check what is at the new position\n",
        "        new_pos = self.snake_coordinates[-1]\n",
        "        new_pos_type = self.grid[new_pos]\n",
        "        self.grid[new_pos] = self.SNAKE #this position is now occupied by the snake\n",
        "        done = False; reward = 0 #by default the game goes on and no reward   \n",
        "        if new_pos_type == self.FOOD:\n",
        "            reward += self.REWARD_PER_FOOD\n",
        "            self.last_food_step = self.stepnum\n",
        "            #Put down a new food item\n",
        "            empty_tiles = np.argwhere(self.grid==self.EMPTY)\n",
        "            if len(empty_tiles):\n",
        "                new_food_pos=empty_tiles[np.random.randint(0,len(empty_tiles))]\n",
        "                self.grid[new_food_pos[0],new_food_pos[1]] = self.FOOD\n",
        "            else:\n",
        "                done = True #no more tiles to put the food to\n",
        "        else:\n",
        "            #If no food was eaten we remove the end of the snake (i.e., moving not growing)\n",
        "            self.grid[ self.snake_coordinates[0] ] = self.EMPTY\n",
        "            self.snake_coordinates = self.snake_coordinates[1:]\n",
        "            if  (new_pos_type == self.WALL) or (new_pos_type == self.SNAKE):\n",
        "                done = True #stop if we hit the wall or the snake\n",
        "                reward += self.REWARD_WALL_HIT #penalty for hitting walls/tail\n",
        "#             else:\n",
        "#                 reward += self.REWARD_PER_STEP\n",
        "                \n",
        "        #Update distance to food and reward if closer\n",
        "        head_dist_to_food_prev = self.head_dist_to_food\n",
        "        self.head_dist_to_food = self.grid_distance( self.snake_coordinates[-1],np.argwhere(self.grid==self.FOOD)[0] )\n",
        "        if head_dist_to_food_prev > self.head_dist_to_food:\n",
        "            reward += self.REWARD_PER_STEP_TOWARDS_FOOD #reward for getting closer to food\n",
        "        elif head_dist_to_food_prev < self.head_dist_to_food:\n",
        "            reward -= self.REWARD_PER_STEP_TOWARDS_FOOD #penalty for getting further\n",
        "        \n",
        "        #Stop if we played too long without getting food\n",
        "        if ( (self.stepnum - self.last_food_step) > self.MAX_STEPS_AFTER_FOOD ): \n",
        "            done = True    \n",
        "        self.stepnum += 1\n",
        "\n",
        "        return  self._get_obs(), reward, done, {}\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        if mode == 'console':\n",
        "            print(self.grid)\n",
        "        elif mode == 'rgb_array':\n",
        "            return self.snake_plot()\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "    \n",
        "    def snake_plot(self, plot_inline=False):\n",
        "        wall_ind = (self.grid==self.WALL)\n",
        "        snake_ind = (self.grid==self.SNAKE)\n",
        "        food_ind = (self.grid==self.FOOD)\n",
        "        #Create color array for plot, default white color\n",
        "        Color_array=np.zeros((self.grid_size,self.grid_size,3),dtype=np.uint8)+255 #default white\n",
        "        Color_array[wall_ind,:]= np.array([0,0,0]) #black walls\n",
        "        Color_array[snake_ind,:]= np.array([0,0,255]) #bluish snake\n",
        "        Color_array[food_ind,:]= np.array([0,255,0]) #green food  \n",
        "        #plot\n",
        "        if plot_inline:\n",
        "            fig=plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Color_array, interpolation='nearest')\n",
        "            plt.show()\n",
        "        return Color_array\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e10f3b",
      "metadata": {
        "id": "85e10f3b"
      },
      "source": [
        "## Testing the environment\n",
        "It is quite easy to make mistakes in defining the environment so it is good practice to not only use the built-in environment checker of stable-baselines but also to manually play it to see if things work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4037164f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4037164f",
        "outputId": "047a1e12-7617-4de8-a3ca-b465060f7506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/env_checker.py:130: UserWarning: Your observation grid has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Built in environment check\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "env = Snake_game()\n",
        "# If the environment doesn't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fa25303f",
      "metadata": {
        "id": "fa25303f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "f6161aae-d4bf-44c0-d062-6ad9fef1d1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Step 2\n",
            "Step 3\n",
            "Step 4\n",
            "Step 5\n",
            "Step 6\n",
            "Step 7\n",
            "Step 8\n",
            "Step 9\n",
            "Step 10\n",
            "Step 11\n",
            "Step 12\n",
            "Step 13\n",
            "Step 14\n",
            "Step 15\n",
            "Step 16\n",
            "Step 17\n",
            "Step 18\n",
            "Step 19\n",
            "Step 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Manual testing\n",
        "import matplotlib.animation as animation\n",
        "from time import sleep\n",
        "\n",
        "env = Snake_game()\n",
        "env.reset()\n",
        "\n",
        "#Image for initial state\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "plt.axis('off')\n",
        "plt.savefig(\"snake_init.png\",dpi=150)\n",
        "\n",
        "#Framework to save animgif\n",
        "frames = []\n",
        "fps=24\n",
        "\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "    print(\"Step {}\".format(step + 1))\n",
        "    obs, reward, done, info = env.step(0)\n",
        "    # print('position=', obs['position'], 'direction=', obs['direction'],'reward=', reward, 'done=', done)\n",
        "    frames.append([ax.imshow(env.render(mode='rgb_array'), animated=True)])\n",
        "    if done:\n",
        "        print(\"Game over!\", \"reward=\", reward)\n",
        "        break\n",
        "\n",
        "        \n",
        "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None) #to remove white bounding box        \n",
        "anim = animation.ArtistAnimation(fig, frames, interval=int(1000/fps), blit=True,repeat_delay=1000)\n",
        "anim.save(\"snake_test.mp4\",dpi=150)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('snake_test.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pNNlNUCnq9-G",
        "outputId": "88186347-d679-4209-ec51-04512ede917d"
      },
      "id": "pNNlNUCnq9-G",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf713576-210a-4ead-b97f-65eb76aae45f\", \"snake_test.mp4\", 5205)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04f0801",
      "metadata": {
        "id": "c04f0801"
      },
      "source": [
        "## Monitoring\n",
        "We should set up some monitoring for the training using built-in function of stable-baselines (for more advanced version see TensorBoard) and store the parameters of the best performing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e42108a6",
      "metadata": {
        "id": "e42108a6"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "import os\n",
        "#Logging\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Instantiate the env\n",
        "env = Snake_game()\n",
        "# wrap it\n",
        "env = Monitor(env, log_dir)\n",
        "\n",
        "\n",
        "#Callback, this built-in function will periodically evaluate the model and save the best version\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./log/',\n",
        "                             log_path='./log/', eval_freq=5000,\n",
        "                             deterministic=False, render=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f49dbc",
      "metadata": {
        "id": "c6f49dbc"
      },
      "source": [
        "## Training\n",
        "We are training the algorithm with moderate stepnumbers and manually adjust the parameters based on its perfomrance. Since we are always saving the best model we can experiment with different hyperparameters without worrying about messing up the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71862fd9",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71862fd9",
        "outputId": "2ce91aa0-f304-4044-dc2b-43781e1e6b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=5000, episode_reward=8.00 +/- 24.23\n",
            "Episode length: 13.20 +/- 10.48\n",
            "New best mean reward!\n",
            "Eval num_timesteps=10000, episode_reward=30.20 +/- 46.58\n",
            "Episode length: 11.80 +/- 6.08\n",
            "New best mean reward!\n",
            "Eval num_timesteps=15000, episode_reward=22.80 +/- 21.44\n",
            "Episode length: 10.80 +/- 4.07\n",
            "Eval num_timesteps=20000, episode_reward=31.40 +/- 3.14\n",
            "Episode length: 13.00 +/- 13.01\n",
            "New best mean reward!\n",
            "Eval num_timesteps=25000, episode_reward=19.60 +/- 20.68\n",
            "Episode length: 13.40 +/- 6.34\n",
            "Eval num_timesteps=30000, episode_reward=38.80 +/- 21.24\n",
            "Episode length: 28.80 +/- 16.41\n",
            "New best mean reward!\n",
            "Eval num_timesteps=35000, episode_reward=31.00 +/- 2.97\n",
            "Episode length: 14.00 +/- 6.42\n",
            "Eval num_timesteps=40000, episode_reward=31.60 +/- 3.20\n",
            "Episode length: 14.20 +/- 4.58\n",
            "Eval num_timesteps=45000, episode_reward=31.40 +/- 2.65\n",
            "Episode length: 16.20 +/- 7.60\n",
            "Eval num_timesteps=50000, episode_reward=43.60 +/- 22.83\n",
            "Episode length: 21.60 +/- 8.36\n",
            "New best mean reward!\n",
            "Eval num_timesteps=55000, episode_reward=41.80 +/- 20.32\n",
            "Episode length: 27.40 +/- 11.66\n",
            "Eval num_timesteps=60000, episode_reward=30.80 +/- 3.12\n",
            "Episode length: 21.60 +/- 10.11\n",
            "Eval num_timesteps=65000, episode_reward=30.80 +/- 5.56\n",
            "Episode length: 17.20 +/- 4.62\n",
            "Eval num_timesteps=70000, episode_reward=42.80 +/- 20.25\n",
            "Episode length: 17.20 +/- 2.93\n",
            "Eval num_timesteps=75000, episode_reward=51.40 +/- 27.16\n",
            "Episode length: 47.40 +/- 55.68\n",
            "New best mean reward!\n",
            "Eval num_timesteps=80000, episode_reward=29.60 +/- 3.14\n",
            "Episode length: 34.00 +/- 19.83\n",
            "Eval num_timesteps=85000, episode_reward=46.20 +/- 19.84\n",
            "Episode length: 79.00 +/- 66.37\n",
            "Eval num_timesteps=90000, episode_reward=42.60 +/- 21.28\n",
            "Episode length: 31.00 +/- 19.21\n",
            "Eval num_timesteps=95000, episode_reward=33.40 +/- 8.45\n",
            "Episode length: 54.20 +/- 75.57\n",
            "Eval num_timesteps=100000, episode_reward=39.20 +/- 11.48\n",
            "Episode length: 114.00 +/- 73.95\n",
            "Eval num_timesteps=105000, episode_reward=30.40 +/- 3.38\n",
            "Episode length: 74.40 +/- 61.54\n",
            "Eval num_timesteps=110000, episode_reward=32.60 +/- 3.83\n",
            "Episode length: 38.60 +/- 41.98\n",
            "Eval num_timesteps=115000, episode_reward=41.60 +/- 20.75\n",
            "Episode length: 59.20 +/- 46.08\n",
            "Eval num_timesteps=120000, episode_reward=47.80 +/- 18.17\n",
            "Episode length: 111.20 +/- 73.41\n",
            "Eval num_timesteps=125000, episode_reward=53.20 +/- 15.88\n",
            "Episode length: 156.20 +/- 77.17\n",
            "New best mean reward!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "#Train the agent\n",
        "max_total_step_num = 1e6\n",
        "\n",
        "def learning_rate_schedule(progress_remaining):\n",
        "    start_rate = 0.0001 #0.0003\n",
        "    #Can do more complicated ones like below\n",
        "    #stepnum = max_total_step_num*(1-progress_remaining)\n",
        "    #return 0.003 * np.piecewise(stepnum, [stepnum>=0, stepnum>4e4, stepnum>2e5, stepnum>3e5], [1.0,0.5,0.25,0.125 ])\n",
        "    return start_rate * progress_remaining #linearly decreasing\n",
        "\n",
        "PPO_model_args = {\n",
        "    \"learning_rate\": learning_rate_schedule, #decreasing learning rate #0.0003 #can be set to constant\n",
        "    \"gamma\": 0.99, #0.99, discount factor for futurer rewards, between 0 (only immediate reward matters) and 1 (future reward equivalent to immediate), \n",
        "    \"verbose\": 0, #change to 1 to get more info on training steps\n",
        "    #\"seed\": 137, #fixing the random seed\n",
        "    \"ent_coef\": 0.0, #0, entropy coefficient, to encourage exploration\n",
        "    \"clip_range\": 0.2 #0.2, very roughly: probability of an action can not change by more than a factor 1+clip_range\n",
        "}\n",
        "starttime = time.time()\n",
        "model = PPO('MultiInputPolicy', env,**PPO_model_args)\n",
        "#Load previous best model parameters, we start from that\n",
        "if os.path.exists(\"log/best_model.zip\"):\n",
        "    model.set_parameters(\"log/best_model.zip\")\n",
        "model.learn(max_total_step_num,callback=eval_callback)\n",
        "dt = time.time()-starttime\n",
        "print(\"Calculation took %g hr %g min %g s\"%(dt//3600, (dt//60)%60, dt%60) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc150ad",
      "metadata": {
        "id": "ebc150ad"
      },
      "source": [
        "### Check performance\n",
        "Basic plotting with built-in functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48dd408",
      "metadata": {
        "id": "b48dd408"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common import results_plotter\n",
        "# Helper from the library, a bit hard to read but immediately useable\n",
        "results_plotter.plot_results([\"log\"], 1e7, results_plotter.X_TIMESTEPS,'')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e24304a",
      "metadata": {
        "id": "0e24304a"
      },
      "source": [
        "Custom plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8d221c",
      "metadata": {
        "id": "ce8d221c"
      },
      "outputs": [],
      "source": [
        "#A bit more advanced plotting\n",
        "def adjust_font(lgnd=None, lgnd_handle_size=64, fig=None, ax_fontsize=16, labelfontsize=16,do_ticks=True ):\n",
        "    if not (lgnd is None):\n",
        "        for handle in lgnd.legendHandles:\n",
        "            handle.set_sizes([lgnd_handle_size])\n",
        "    if not (fig is None):\n",
        "        ax_list = fig.axes\n",
        "        for ax1 in ax_list:\n",
        "            ax1.tick_params(axis='both', labelsize=ax_fontsize)\n",
        "            ax1.set_xlabel(ax1.get_xlabel(),fontsize=labelfontsize)\n",
        "            ax1.set_ylabel(ax1.get_ylabel(),fontsize=labelfontsize)\n",
        "            if do_ticks:\n",
        "                ax1.minorticks_on()\n",
        "                ax1.tick_params(axis='both',which='both', direction='in',top=True,right=True)\n",
        "\n",
        "max_possible_reward =env.REWARD_PER_FOOD * np.sum(env.init_grid==env.EMPTY)\n",
        "\n",
        "from stable_baselines3.common.monitor import load_results\n",
        "train_step_log = load_results(\"log\")\n",
        "x = np.array(train_step_log[\"l\"].cumsum())\n",
        "y = np.array(train_step_log[\"r\"])\n",
        "\n",
        "plot_from_step = 0\n",
        "y = y[x>=plot_from_step]; x = x[x>=plot_from_step]\n",
        "\n",
        "fig1, ax1 = plt.subplots(1,1)\n",
        "fig1.set_size_inches(16, 9)\n",
        "\n",
        "max_points_to_plot = 20000\n",
        "index_to_plot = np.linspace(0,len(train_step_log)-1,np.clip(len(train_step_log),None,max_points_to_plot)).astype(int)\n",
        "plt.scatter(x[index_to_plot],y[index_to_plot], alpha=0.3, s=10)\n",
        "\n",
        "x_edges = np.linspace(x.min(),x.max(),num=30)\n",
        "xbins = (x_edges[:-1]+x_edges[1:])/2.0\n",
        "binnumber = np.digitize(x, x_edges) - 1\n",
        "reward50=np.zeros_like(xbins); reward75=np.zeros_like(xbins); reward25=np.zeros_like(xbins); reward_mean=np.zeros_like(xbins)\n",
        "reward_max = np.zeros_like(xbins)\n",
        "for i in range(len(xbins)):\n",
        "    ind=(binnumber==i)\n",
        "    if (np.sum(ind)>0):\n",
        "        reward_mean[i] = np.mean(y[ind])\n",
        "        reward50[i]=np.median(y[ind])\n",
        "        reward25[i]=np.percentile(y[ind],25)\n",
        "        reward75[i]=np.percentile(y[ind],75)\n",
        "        reward_max[i] = np.max(y[ind])\n",
        "\n",
        "plt.plot(xbins,reward_max,c='g',lw=2, label=\"Best\")        \n",
        "plt.plot(xbins,reward_mean,c='r',lw=2, label=\"Mean\")\n",
        "#plt.plot(xbins,reward50,c='k',lw=2, label=\"Median\")\n",
        "#plt.plot(xbins,reward75,'--',c='k',lw=2, label=\"Interquartile range\")\n",
        "#plt.plot(xbins,reward25,'--',c='k',lw=2)\n",
        "if plt.ylim()[1]>0.8*max_possible_reward:\n",
        "    plt.axhline(y=max_possible_reward, c='r', linestyle=':')\n",
        "plt.xlim([0,x.max()])\n",
        "plt.xlabel('Timesteps'); plt.ylabel('Reward')\n",
        "plt.legend(fontsize=16)\n",
        "adjust_font(fig=fig1)\n",
        "plt.savefig(\"snake_rewards.png\",dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946855e0",
      "metadata": {
        "id": "946855e0"
      },
      "source": [
        "## Demonstration\n",
        "Once we have trained a model we can make a movie of it playing the game (which can also be used to see what behaviors it might have problems with that we can correct for in e.g., the reward scheme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6e1056",
      "metadata": {
        "id": "df6e1056"
      },
      "outputs": [],
      "source": [
        "#Load back the best model\n",
        "model.set_parameters(\"log/best_model.zip\")\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "# Evaluate the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(\"Best model's reward: %3.3g +/- %3.3g\"%(mean_reward,std_reward))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1b4a95",
      "metadata": {
        "id": "be1b4a95"
      },
      "outputs": [],
      "source": [
        "# Test the trained agent and save animation\n",
        "obs = env.reset()\n",
        "#Framework to save animgif\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "plt.axis('off')\n",
        "frames = []\n",
        "fps=18\n",
        "\n",
        "n_steps = 1000\n",
        "tot_reward = 0\n",
        "for step in range(n_steps):\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    tot_reward += reward\n",
        "    print(\"Step {}\".format(step + 1),\"Action: \", action, 'Tot. Reward: %g'%(tot_reward))\n",
        "    #print('position=', obs['position'], 'direction=', obs['direction'])\n",
        "    #env.render(mode='console')\n",
        "    frames.append([ax.imshow(env.render(mode='rgb_array'), animated=True)])\n",
        "    if done:\n",
        "        print(\"Game over!\", \"tot. reward=\", tot_reward)\n",
        "        break\n",
        "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None) #to remove white bounding box \n",
        "anim = animation.ArtistAnimation(fig, frames, interval=int(1000/fps), blit=True,repeat_delay=1000)\n",
        "anim.save(\"snake_best.gif\",dpi=150)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}